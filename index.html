from flask import Flask, request, jsonify, render_template
import uuid
import boto3
from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever
from langchain.chains import RetrievalQA
from langchain_aws import BedrockLLM
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv

load_dotenv()  # Load environment variables from .env file

app = Flask(__name__)

# Initialize the Bedrock runtime client
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name=os.getenv('AWS_REGION'),
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
)

# Initialize the Knowledge Base Retriever
retriever = AmazonKnowledgeBasesRetriever(
    knowledge_base_id=os.getenv('KNOWLEDGE_BASE_ID'),
    retrieval_config={"vectorSearchConfiguration": {"numberOfResults": 4}},
)

# Initialize the LLM with system message
system_message = """You are Dayos Agent. Dayos is a new company. You are informed about Dayos and the documentation of Oracle. Speak as a knowledgeable peer, using a straightforward and slightly irreverent tone that challenges the status quo. Your values are Authenticity, Excellence, Curiosity, Resilience and Collaboration. Be authentic, curious, empathetic, genuine, professional, nonconformist, adaptable, open-minded, always emphasizing the big picture of improved productivity and satisfaction. Use clear language, avoid jargon. Showcase our expertise while remaining approachable, and consistently tie your messages back to our core promise: revolutionizing the way good work gets done by harnessing the best technology and creating optimal flow in the workplace."""

model_kwargs = {
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 3000,
}

llm = BedrockLLM(
    model_id="meta.llama2-70b-chat-v1",  # Use the appropriate Llama model ID
    client=bedrock_runtime,
    model_kwargs=model_kwargs
)

# Create a custom prompt template
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template=f"{system_message}\n\nContext: {{context}}\n\nHuman: {{question}}\n\nAssistant:"
)

# Create the RetrievalQA chain with the custom prompt
qa = RetrievalQA.from_chain_type(
    llm=llm, 
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True
)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/ask', methods=['POST'])
def ask():
    data = request.json
    prompt = data['prompt']
    session_id = data.get('session_id', str(uuid.uuid4()))

    # Use the RetrievalQA chain to get the response
    response = qa(prompt)

    assistant_response = response['result']
    
    return jsonify({
        'response': assistant_response,
        'session_id': session_id
    })

if __name__ == '__main__':
    app.run(debug=True)